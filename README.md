# 목적
(가능하다면) 매일 논문 한편씩 읽는다.

# 참고
[https://github.com/jungwoo-ha/WeeklyArxivTalk/](https://github.com/jungwoo-ha/WeeklyArxivTalk/)


# 앞으로 읽을 논문 list
## Neurips 2020
### Tutorial

*   Equivariant Networks
*   Where Neuroscience meets AI (And What’s in Store for the Future)
*   Advances in Approximate Inference
*   Practical Uncertainty Estimation and Out-of-Distribution Robustness in Deep Learning
*   Policy Optimization in Reinforcement Learning
*   Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities

### Oral

*   Equivariant Networks for Hierarchical Structures
*   Deep Transformation-Invariant Clustering
*   ~~Escaping the Gravitational Pull of Softmax~~
*   ~~Rethinking Pre-training and Self-training~~
*   Do Adversarially Robust ImageNet Models Transfer Better?
*   Contrastive learning of global and local features for medical image segmentation with limited annotations
*   Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning
*   On the training dynamics of deep networks with L2 regularization
*   Towards a Better Global Loss Landscape of GANs
*   Is normalization indispensable for training deep neural network?

### Spotlight

*   Debiased Contrastive Learning
*   The Autoencoding Variational Autoencoder
*   Joint Contrastive Learning with Infinite Possibilities
*   What Do Neural Networks Learn When Trained With Random Labels?
*   H-Mem: Harnessing synaptic plasticity with Hebbian Memory Networks
*   A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
*   Large-Scale Adversarial Training for Vision-and-Language Representation Learning
*   Measuring Robustness to Natural Distribution Shifts in Image Classification
*   The Complete Lasso Tradeoff Diagram
*   Adversarial Training is a Form of Data-dependent Operator Norm Regularization
*   Most ReLU Networks Suffer from ℓ2ℓ2 Adversarial Perturbations
*   What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation
*   

### Poster

*   ~~Unsupervised Data Augmentation for Consistency Training~~


